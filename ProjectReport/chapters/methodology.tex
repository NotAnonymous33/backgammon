\chapter{Methodology}

This chapter describes the processes, techniques, and models used to design, implement, test, and evaluate the backgammon system.

\section{Overall System Design}
The system follows a three tier architecture, consisting of the presentation layer, application layer, and data layer. The presentation layer is responsible for the web client (implemented in React/TypeScript); the application layer consists of the game logic and AI (implemented in C++, Cython, and Python); and the data layer is responsible for the database (implemented SQLite). To interface between the presentation and application layer, a REST API is used to join and connect to a game, after which a WebSocket connection is established to allow for real-time communication between the client and server. To interface between the application and data layer, a Flask server, which uses SQLAlchemy as its ORM, manages database interactions.

By using a three tier architecture, the frontend, backend, and database can be developed independently. This allows for each layer to be tested and evaluated separately, while also allowing large architectural changes within each layer as long as the interfaces between layers remain constant. This proved useful when developing the AI, as some Python sections were replaced with languages which had higher performance. The three tier architecture also has security benefits; by forcing every request to flow through each layer, data can be controlled and validated. This stops clients from manipulating game states directly, preventing cheating and unwanted data manipulation.

There are however downsides to this architecture. The main downside is the increased complexity of the system, as there are more components to manage and maintain. Often this leads to similar code being written in multiple languages in different layers. For example, checks for a valid move are implemented in both the frontend and backend. This is discussed further in section \ref{sec:userinteractions}.  
Another downside is the increased latency of the system; where there is a barrier between layers, latency is naturally introduced. 

\section{Frontend Design}
TODO write out this idea: Chess is way more popular than backgammon, naturally this has led to popular chess websites being much more modern and better(?). Because of this, development of the frontend was heavily inspired by popular chess websites such as chess.com and lichess.org

Since chess has a much larger playerbase than backgammon, its online platforms have been developed much faster and more frequently. Websites like \href{https://www.chess.com/}{Chess.com} and \href{https://lichess.org/}{Lichess.org} were used to inspire development of the backgammon client, due to their modern, interactive interfaces.

\subsection{React and TypeScript}
The frontend, which is responsible for rendering the game board, handling user interactions, and communicating with the backend, was implemented in React/TypeScript. The frontend uses a REST API to join and connect to a game, after which a WebSocket connection is established to allow for real-time communication between the client and server. 

React/TypeScript was chosen for several reasons. React is the most popular frontend framework \cite{state_of_js_2024}, resulting in there being a large community and ecosystem, such as state management libraries (e.g. Redux) and UI component libraries (e.g. Shadcn).

TypeScript is a superset of JavaScript that adds static typing, which helps catch errors at compile time rather than runtime, and improves the developer experience by providing better autocomplete features. 
This is especially useful in a large codebase where the types act as documentation, helping to ensure that the code is more maintainable and less error-prone.


\subsection{Game Rendering}
The game board is rendered using Scalable Vector Graphics (SVG), which was chosen over other rendering methods, such as the HTML canvas element, as its format allows for easily manipulation of individual components on the board. This is useful for implementing features such as highlighting valid moves, showing the current player, and individual checker manipulation. As a vector format, SVG scaled perfectly across all devices and resolutions, increasing accessibility by being identical on all devices
Unlike canvas, SVG natively supports event listeners for clicking and hovering, and CSS animations can be easily added without requiring manual animation logic.
\subsection{Handling User Interactions}
\label{sec:userinteractions}
To allow for user preference, both drag-and-drop and click-and-move have been implemented. 
Drag-and-drop is implemented by using mouse event handlers (\verb|onMouseDown|, \verb|onMouseMove|, and \verb|onMouseUp|) to track the position of the mouse and update the position of the checkers accordingly. When checkers are clicked, coordinates of the mouse, the checker, and the board are used to determine the start and end positions of the move. A move is registered when the user releases the mouse, and the distance between the current checker and the closest triangle is less than a certain threshold. 
Click-and-move is implemented by using \verb|onMouseDown| to track the position of the checkers and update the position of the checkers accordingly. Generally, \verb|onClick| is used to handle user clicks, however \verb|onMouseDown| has many advantages over \verb|onClick|. Human computer interaction research \cite{mousedown1} \cite{mousedown2} has shown that responses under 100ms are perceived as instant. By using \verb|onMouseDown|, the time between a user's action and the system's response is greatly reduced, vastly increasing the perceived responsitveness of the system.

TODO talk about client side move validation

\subsection{Real time Communication}
When the user joins a game, they are connected to the server through a socket. This allows for real-time updates and allows the server to send messages to clients without the need for the client to make a specific request. The only alternative to this is some form of polling, where the client repeatedly requests updates from the server. This is inefficient leads to unnecessary latency and server load.

\section{Backend Design}

\begin{definition}[Object-Relational Mapping (ORM)]
Object-relational mapping is a technique used to convert data between an object oriented programming language and a database, allowing database interactions to be called directly.
\end{definition}

The backend is responsible for handling the game logic, state management, AI, and database interactions. The backend is implemented in Python using Flask for HTTP routing, Sockets for communication, and SQLAlchemy as an ORM to store and retrieve game states from an SQLite database. 
This layered design allows for the backend to be easily extended and modified, as each layer is responsible for a specific task. (game logic, AI, data handling).

\subsection{API Endpoints}
The server exposes a single REST endpoint to create games before handing off to WebSocket communication:

\textbf{POST /api/new\_game}: Generates a unique 5-character room code, instantiates a new \texttt{Board} in memory, which persists by being stored in the database. This allows for games to be resumed after a range of events, such as server restarts, or clients disconnecting. 

After initialization, all game actions, such as rolling dice, making moves, joining/leaving rooms—are handled over Socket events (Section~\ref{sec:room-management}).


\subsection{Room Management}
\label{sec:room-management}
Game sessions are tracked in a dictionary which is stored in memory and mapped to each game room by room code. The following Socket events are handled by the server:
\begin{itemize}
\item \texttt{join\_room}: Validates that the requested side is unoccupied, registers either a human (tracked by \texttt{sid}) or AI player (stores \texttt{ai\_model}), calls \texttt{join\_room(room\_code)}, and responds with the current board state via \texttt{"update\_board"}.
\item \texttt{leave\_room} / \texttt{disconnect}: Removes the client from the room, emits a \texttt{"left\_room"} event, and leaves residual state in the dictionary (to preserve ongoing matches during client disconnects).
\item \texttt{move}: Validates the move against the current board state, and if valid, updates the board, the corresponding board entry in the database, and emits and emits an \texttt{"update\_board"} event which sends the new board to all clients. If the move is invalid, an \texttt{"error"} event is emitted.
\item \texttt{roll\_dice}: Rolls the dice, updates the board, and emits an \texttt{"update\_dice"} event. This event sends the new dice values, invalid dice values, and valid moves to all clients in the room. 
If the player is an AI, the \texttt{ai\_move} function is called to handle the move asynchronously. This allows the AI to make its move, which can take substantial time and computation, without blocking the main event loop.
\end{itemize}
Turn enforcement: Every \texttt{move} or \texttt{roll\_dice} event compares the socket client's session id (SID) with the stored SID in the database. Unauthorised actions trigger an \texttt{"error"} event, preventing spoofed commands or out of turn moves.

When data is updated, the server emits events to clients. Clients listen for these events to stay synchronised with the server's game state.
The following socket events are emitted by the server:
\begin{enumerate}
    \item \texttt{joined\_room}: Emits a message to players that a new client has successfully joined the game.
    
    \item \texttt{update\_board}: After changes have been made to the board state, such as after dice rolls, player moves, AI moves, a new board is sent to players. 

    \item \texttt{update\_dice}: After a player rolls the dice, the updated dice is sent to all players.

    \item \texttt{error}: When an invalid action is attempted, such as an invalid move, or attempting to re-roll dice, an error is emitted to be displayed to players.
\end{enumerate}


\subsection{Database Schema and Interaction}
Persistence of game state is achieved through a single SQLAlchemy model, whose schema is shown in Figure~\ref{fig:game-schema}.

\begin{figure}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Column}             & \textbf{Type}          & \textbf{Description}               \\ \hline
\texttt{id}                 & Integer PK             & Primary key         \\ \hline
\texttt{room\_code}          & String(5), unique      & Game identifier                    \\ \hline
\texttt{positions}          & PickleType              & Board positions array   \\ \hline
\texttt{dice}, \texttt{invalid\_dice} & String(4)             & Current and invalid dice rolls     \\ \hline
\texttt{turn}               & Integer                & 1 = white to move, 0 = black      \\ \hline
\texttt{white\_bar}, \texttt{black\_bar} & Integer       & Checkers on the bar                \\ \hline
\texttt{white\_off}, \texttt{black\_off} & Integer      & Checkers borne off                 \\ \hline
\texttt{rolled}, \texttt{game\_over}  & Boolean        & Dice roll flag, game end flag      \\ \hline
\texttt{white\_ai\_model}, \texttt{black\_ai\_model} & String(20) & AI agent identifiers            \\ \hline
\end{tabular}
\caption{SQLAlchemy model schema}
\label{fig:game-schema}
\end{figure}

Two helper functions manage state transformations:
\begin{itemize}
\item Adding a board to the database: Creates and commits a new row, representing the initial state.
\item Updating a board in the database: Queries the latest row by room code, mutates its fields in place, and commits. One downside of this pattern is that intermediate states and moves are not stored, which could be useful for debugging or replaying games.
\end{itemize}

Atomic transactions and session management ensure that either all state changes occur together, preserving consistency. Queries always fetch the most recent state, guaranteeing that all clients operate on a synchronized game.

\section{Game Logic}
In this section, the core game logic is described, including the board representation, move generation and validation, and the interfacing between C++ and Python. C++ was used for the core game logic due to its performance advantages, which was then bound to Python using pybind11. 
\subsection{State Representation and Methods}
A \texttt{Board} class is used to store the game state, and contains the following attributes:
\begin{itemize}
\item \texttt{positions}: an array where each index corresponds to a point on the backgammon board. Positive values indicate the number of white checkers; negative values indicate the number of black checkers.
\item \texttt{white\_bar, black\_bar}: counters for checkers that have been hit and are waiting to re-enter.
\item \texttt{white\_off, black\_off}: counters for checkers that have been successfully borne off.
\item \texttt{dice}: captures the current roll.
\item \texttt{invalid\_dice}: stores the rolled dice values that are unusable in the current turn.
\item \texttt{valid\_moves}: stores every permissible sequence of moves for the active player in the current turn.
\item Game status flags: \texttt{rolled} (whether the dice have been rolled to prevent players from re-rolling dice), \texttt{passed} (whether all of one player's checkers have passed the opponent's checkers.  If this is true, no more pieces can be hit. ), \texttt{game\_over}, and \texttt{turn} (1 for White, -1 for Black).
\end{itemize}
The use of a single integer array to represent the board state allows for efficient storage and manipulation of the game state, as opposed to storing checker objects, which would increase memory usage and complexity with no significant benefit.

The following support methods are implemented to allow for easier implementation of AI agents:
\begin{itemize}
    \item \texttt{calc\_white\_left() / calc\_black\_left()}: computes a pip count (distance remaining) for each player by summing the product of checker count and point distance from bearing-off.
    \item \texttt{has\_passed()}: Determines passed status.
    \item \texttt{game\_over()}: returns true when either \texttt{white\_off} or \texttt{black\_off} reaches 15.
    \item \texttt{copy\_state\_from()}: copies a given board state into the current instance.
    \item \texttt{convert()}: serializes the entire C++ state into a Python dictionary (\texttt{py::dict}), allowing the Python layer to load and create database entries with minimal overhead.
    \item Clone and copy constructors: creates a copy of the board state, overriding the default python constructors, and allowing for exploration without altering the original state. This is useful for AI agents that need to explore multiple possible moves without modifying the original state.
\end{itemize}

\subsection{Move Generation and Validation}
As mentioned in section~\ref{sec:rules}, there are multiple rules on what moves can be made. Since each turn consists of moving multiple checkers, the move generation and validation process is complex. Each moving of a single checker cannot be checked individually, in the cases where a player has to maximise the number of checkers moved; or has to maximise the die used. Therefore, rather than checking each moves validity, a list of all possible moves is generated (which is also extremely useful for the AI agents).
The move generation process consists of two parts: identifying which dice can be used, then generating all possible moves.
\begin{enumerate}
    \item \textbf{Invalid Dice Detection}: Dice rolls that cannot be used to make any valid moves on the current board are first identified. Pseudocode for the function can be seen in algorithm \ref{alg:getInvalidDice}. A recursive helper function \texttt{evaluate\_dice()} is used to determine the unusable dice, as shown in algorithm \ref{alg:evaluateDice}. This function tries all possible ways to use the dice, applying one move at a time. If more dice can be used after a move, the function calls itself after removing the used dice from the list. If a point is reached where there are unused dice and no valid moves, the dice which result in the more dice being used, and higher value dice being used, are marked as valid, and the rest are marked as invalid. This can be extremely expensive, so optimisations are necessary. A very impactful optimisation is to return immediately if there is a single instance where all dice can be used. Since this is generally the case, this optimisation is very effective. 
    Once all invalid dice have been identified, the invalid dice are removed from the dice list and added to \texttt{invalid\_dice}.

    \item \textbf{Depth First Search for Move Sequences}: With invalid dice removed, a depth first search is performed on all the single die moves to create a list of full turn sequences, as shown in algorithm \ref{alg:getValidMoves}. Similarly to the invalid dice detection, a recursive helper function \texttt{dfs()} (as shown in algorithm \ref{alg:dfs}) is used to recursively generate all possible moves. In the general case, the function calls \texttt{get\_single\_moves()} (as shown in algorithm \ref{alg:singleMoves}) to list every legal single move that can be made from the current board state. For each of these moves, the function clones the current board state, applies the move, and removes the used die from the list. The \texttt{dfs()} function then calls itself with the new board state and the remaining dice. This process continues until all there are no dice remaining (base case), or no further moves can be made. For the base case, if all dice have been used, the function returns the list of moves it has calculated.
\end{enumerate}

\begin{algorithm}[H]
    \caption{Get Invalid Dice}
\label{alg:getInvalidDice}
    \begin{algorithmic}
    \State \textbf{Input:} Current dice list, board configuration
    \State \textbf{Output:} List of invalid dice
    \State \text{invalid\_dice} $\gets$ \text{dice}
    \State \text{max\_length} $\gets$ 0
    \State \text{max\_die} $\gets$ 0
    \State \text{board\_copy} $\gets$ \text{copy of board}
    \State
    \Comment{Attempt to use all dice in some order}
    \If{evaluate\_dice(board\_copy, dice, [], max\_length, max\_die, invalid\_dice)}
        \State \textbf{Return} [] \Comment{All dice are useable}
    \Else \Comment{Some dice are unusable and are removed}
        \State \text{result} $\gets$ invalid\_dice
        \For{each die in result}
            \State \text{remove the first occurrence of die from dice}
        \EndFor
        \State \textbf{Return} result
    \EndIf
    \end{algorithmic}
    
\end{algorithm}

\begin{algorithm}[H]
    \caption{Evaluate Dice} \label{alg:evaluateDice}
    \begin{algorithmic}
    \State \textbf{Input:} board, remaining\_dice, used\_dice, max\_length, max\_die, invalid\_dice
    \State \textbf{Output:} True or False
    \State
    \If{remaining\_dice is empty} \Comment{Base case where all dice have been used}
        \State \textbf{Return} True
    \EndIf
    \State
    \If{size(used\_dice) $>$ max\_length} \Comment{A new optimal set of dice has been found}
        \State max\_length $\gets$ size(used\_dice)
        \State invalid\_dice $\gets$ remaining\_dice
    \ElsIf{size(used\_dice) = max\_length and used\_dice is not empty}
        \State current\_max $\gets$ max element of used\_dice
        \If{current\_max $>$ max\_die}
            \State max\_die $\gets$ current\_max
            \State invalid\_dice $\gets$ remaining\_dice
        \EndIf
    \EndIf
    \State
    \If{size(remaining\_dice) = 1} \Comment{Move does not need to be played, only checked}
        \State die $\gets$ remaining\_dice[0]
        \For{each possible move on board using die}
            \If{move is valid}
                \State update max\_length, max\_die, invalid\_dice accordingly
                \State \textbf{Return} True
            \EndIf
        \EndFor
        \State \textbf{Return} False \Comment{There are no possible moves}
    \EndIf
    \State
    \For{each valid single move m on board using die}
        \State new\_board $\gets$ copy of board
        \State apply move m on new\_board
        \State new\_remaining $\gets$ remaining\_dice without the die that led to move m
        \State new\_used $\gets$ used\_dice plus die that led to move m
        \If{evaluate\_dice(new\_board, new\_remaining, new\_used, max\_length, max\_die, invalid\_dice) is True}
            \State \textbf{Return} True
        \EndIf
    \EndFor
    \State \textbf{Return} False
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Get Valid Moves}
    \label{alg:getValidMoves}
    \begin{algorithmic}
    \State \textbf{Input:} board
    \State \textbf{Output:} List of valid moves
    \State
    \State valid\_moves $\gets$ empty list \Comment{Initialize the list for valid moves}
    \State board\_copy $\gets$ copy of Board \Comment{Create a copy of the current board}
    \State valid\_moves $\gets$ dfs(board\_copy, []) \Comment{Call DFS to find valid moves}
    \State \textbf{Return} valid\_moves \Comment{Return the list of valid moves}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Depth First Search (DFS) for Valid Sequences}
    \label{alg:dfs}
    \begin{algorithmic}
    \State \textbf{Input:} board, prev\_moves
    \State \textbf{Output:} List of valid sequences of moves
    \State
    \State \text{// If no dice left, return the one completed sequence (if any)} 
    \If{dice is empty}
        \If{prev\_moves is not empty}
            \State \textbf{Return} [ prev\_moves ] \Comment{Return the completed sequence of moves}
        \Else
            \State \textbf{Return} [] \Comment{Return an empty list if no moves were made}
        \EndIf
    \EndIf
    \State
    \State \text{// If exactly one die remaining, check for valid moves rather than making moves:} TODO make comments consistent
    \If{length(dice) == 1}
        \State results $\gets$ empty list \Comment{Initialize the list to store valid sequences}
        \For{each move in get\_single\_moves(min\_point)}
            \State results.APPEND(prev\_moves + [move]) 
        \EndFor
        \State \textbf{Return} results \Comment{Return the list of completed sequences with the one remaining move}
    \EndIf
    \State
    \State \text{// Otherwise, try every single move and recurse:}
    \State results $\gets$ empty list \Comment{Initialize the results list to store valid sequences}
    \For{each move m in get\_single\_moves(min\_point)}
        \State board\_copy $\gets$ copy of board \Comment{Create a copy of the current board}
        \State make move m on the board
        \Comment{Apply the move to the copied board} TODO remove minpoint
        \State sequences $\gets$ dfs(board\_copy, prev\_moves + [move]) \Comment{Recurse with the updated board and moves}
        \State results.EXTEND(sequences) \Comment{Add the resulting sequences to the final list}
    \EndFor
    \State \textbf{Return} results \Comment{Return the list of all valid sequences found}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Get Single Moves}
    \label{alg:singleMoves}
    \begin{algorithmic}
    \State \textbf{Input:} board, dice
    \State \textbf{Output:} Set of valid moves
    \State
    \State moves $\gets$ empty set \Comment{Initialize the set of valid moves}
    \State
    \If{ there are pieces on the bar}
        \For{each die d in dice}
            \If{die d can be used to re-enter}
                \State Add move corresponding to re-entering with die d to moves set
            \EndIf
        \EndFor
        \State // If there are checkers on the bar, re-entering is the only valid type of move. 
        \State // Therefore normal moves do not need to be checked, so the function can return early
        \State \textbf{Return} moves
    \EndIf
    
    \State
    \State \text{// normal moves} 
    \For{start in max(min\_point, 0) $\dots$ 23}
        \For{each die in dice}
            \State end $\gets$ start + die \Comment{Calculate the end position based on the die roll}
            \If{is\_valid(start, end)}
                \State moves.ADD( (start $\to$ end) ) \Comment{Add the valid move from start to end}
            \EndIf
        \EndFor
    \EndFor

    \State \textbf{Return} moves \Comment{Return the set of all valid moves}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Executing Moves}
When a player chooses a move sequence, the engine proceeds as follows:
\begin{enumerate}
\item Validate the chosen sequence by checking if it is in \texttt{valid\_moves}.
\item Sequentially apply each move: decrement or increment \texttt{positions}, send hit checkers to the bar by incrementing \texttt{white\_bar} or \texttt{black\_bar}, and handle bearing off by incrementing \texttt{white\_off} and \texttt{black\_off}.
\item Remove corresponding dice values, by calculating the distance checkers have travelled. When bearing off, it is possible to bear off a checker from a point that is not equal to the die value, in which case the largest die is removed.
\item Recompute pip counts (\texttt{white\_left}, \texttt{black\_left}), update \texttt{passed} status, and detect \texttt{game\_over} if a player has borne off all pieces.
\item If no dice remain, or the player is forced to pass, the turn is swapped, \texttt{rolled} is set to false, and the dice are cleared.
\end{enumerate}


\subsection{Optimisations}
\paragraph{Pybind11}
Since board methods can be called millions of times during complex AI learning and play, it is important that they are as fast as possible. This resulted in the decision to write the board class in C++ (which is many times faster than Python), and bind it to Python using pybind11. By exposing  C++ class methods, significant performance improvements can be achieved, while maintaining the flexibility of Python for higher level logic, and still being able to use Python's extensive libraries for AI implementations.

\paragraph{Single die shortcut}
During the process of finding invalid dice and checking for valid moves, when there is only one die remaining, the function can return immediately after finding a move, rather than making the move and reaching the base case. As a result, all leaf nodes of the tree are not explored, and the function can prune these branches. Due to the large number of possible single moves, and the frequency of this case, this optimisation leads to significant performance improvements (approximately 67.5\% faster). The main reason this results in large speed improvements is the reduced number of deep copying, which is by far the most expensive primitive operation. 


\paragraph{Deep copying}
Often, the board state needs to be copied (e.g. to evaluate different moves), so that the original state is not modified. Especially during Monte Carlo Tree Search, this function can be repeated thousands of times per second, so it is important to reduce the cost of this operation. In Python, each attribute exists as a reference to a \texttt{PyObject}, rather than a data type such as an integer or array. This allows Python to be more flexible with its type restrictions, but also results in extremely expensive variable operations. When the board state is copied, every attribute must be allocated a new \texttt{PyObject}, and each element of the attribute (for lists) must also be checked for references to other sub-\texttt{PyObjects} (since everything is a \texttt{PyObject}). This results in many dynamic type checks, expensive memory allocations, and garbage collection cycles, despite types in the board being constant, and the size of the board being constant.

In contrast to this, C++ has a fixed type system, which results in elements being copied in far fewer operations, as data is stored contiguously rather than as references to separate memory locations. Without unnecessary type checks and garbage collection cycles, C++ is able to copy the board state much faster than Python, as shown in table \ref{tab:copy-times}. By overriding the default copy methods in Python with implementations in C++, the copy operation could be performed approximately 20 times faster.

\begin{table}[ht]
  \centering

  
  \begin{tabular}{@{}lS[table-format=2.3]@{}}
    \toprule
    {Operation}                  & {Time (s)} \\
    \midrule
    Python default \texttt{deepcopy}      & 16.916 \\
    Python overridden \texttt{deepcopy}   &  1.296 \\
    C++ default \texttt{deepcopy}         &  1.700 \\
    C++ overridden \texttt{deepcopy}      &  0.866 \\
    \bottomrule
  \end{tabular}
    \caption{State copy timings over 1 million iterations}
    \label{tab:copy-times}
\end{table}




\paragraph{Move Ordering Constraint} 
\label{sec:moveordering}
As described by Lishout et al. \cite{mctsinbackgammon}, a problem appears when calculating possible moves where identical positions can be generated through different sequences of moves, which is especially problematic when there are doubles. Rather than computing these positions and removing them, it is much faster to simply not compute them to begin with. 


Instead of naively exploring every ordered sequence of single moves, an ordering constraint on which points a checker can be moved to is implemented. After a checker is moved from point \texttt{i}, only subsequent moves where the starting point is greater than or equal to \texttt{i} are considered (less than \texttt{i} for black). This prunes many branches in the tree, particularly when doubles have been rolled, resulting in valid moves being calculated 5.9 times faster \footnote{This was tested by setting the dice of the same boards 10,000 times and recalculating valid moves. Unordered tree with repeated nodes: 10.935s. Ordered tree with unique nodes: 1.828s}.

Example: you have dice [2,4] and one checker on point 3 and another on 5.

Branch A: move from 3→5 (using the 2), then 5→9 (using the 4).

Branch B: move from 5→9 (4), then 3→5 (2).
Both end with one checker on 5 and one on 9—but in Branch B the second move would start at 3, which is < 5, so we skip it.

There are however, drawbacks to this optimisation, particularly for the users. When playing, they are also constrained by the move ordering constraint, leading to unintuitive behaviour and a poor user experience. 

While the optimisations were being tested, CProfile was used extensively to identify bottlenecks and measure speed improvements.


\section{AI Design and Architecture}
During the implementation of AI agents, multiple decisions were made, which are outlined in this section.


\subsection{Heuristic Agent}
\label{sec:heuristic}
The Heuristic Agent evaluates board positions as a linear combination of the following features, which were chosen based on backgammon strategy literature:
\begin{itemize}
    \item Difference in pip count
    \item Control of home, inner, and outer board
    \item Number of blots
    \item Length of the longest prime
    \item Number of checkers on the bar
\end{itemize}
Terminal states are handled by returning large constants ($\pm 1\times 10^6$).

In order to choose a move, the agent searches through all possible moves, evaluating each resulting state, and picks the move which corresponds to the highest value state. 

To improve the performance of the agent, stochastic gradient descent using self-play was used to find the optimal weights. The loss function was defined as 
\begin{equation}
    L(\mathbf{w})=E\left[(T-H(s, \mathbf{w}))^2\right]
\end{equation}
where $T$ is the target value (1 for a win, -1 for a loss), $H(s, \mathbf{w})$ is the heuristic value of the state $s$ with weights $\mathbf{w}$. 10,000 games were split into batches of 250 games, with the learning rate $\eta$ set to 0.01. 
The weights were updated using the Adam optimiser, which computes individual learning rates for each weight based on their individual gradients. 
Because the Adam optimiser accounts for momentum and bias-correction, it is often the best overall choice for an optimiser \cite{adam1}\cite{adam2}. 

\subsection{Monte Carlo Agent}
The Monte Carlo agent uses a time budgeted MCTS algorithm as described in section \ref{sec:mcts} to determine the best move to make. Users are able to choose between "fast", "standard", and "hard" agents, which have simulation time limits of 1 second, 5 seconds, and 10 seconds respectively. In the case that a double is rolled, the agent's time budget is doubled to account for the increased number of possible moves. The implementation of the MCTS agent is structured into two layers: the MCTS algorithm itself, responsible for selection, expansion, simulation, and backpropagation; and a higher level wrapper which is responsible for managing the game state, game tree, time management, and move selection.

In order to make the MCTS as fast as possible, Cython was chosen as the language to implement the MCTS algorithm, providing a balance of the performance C provides, and the flexibility of Python. Cython is a superset of Python which allows for C types and compilation, resulting in performance improvements. Simiarly to pybind11, by adding types to variables, Python is able to bypass certain type checks, and can allocate memory more efficiently. 
Although slower than a pure C++ implementation bound to Python using pybind11, Cython is able to call python functions and libraries, allowing MCTS to be combined with other models. 

\subsubsection{Move Selection and Initialisation}
When the agent is called, it is provided a copied instance of the board. In edge cases, where the agent has no moves, or only one move, the agent will return immediately. The agent then checks the current stored game tree for a matching node corresponding to the current board state. If a matching node is found, it is set as the root node, preserving statistics and child nodes. By using previously computed nodes, computation from previous iterations is not wasted. If a matching node is not found, the agent will create a new node with fresh statistics. The underlying MCTS algorithm is then called and runs for the specified time budget. 


Implementing move ordering from section \ref{sec:moveordering}, also results in much more accurate statistics. By forcing identical board positions, regardless of the move sequence which produced them, simulations are not split across duplicate nodes with the same board positions. As a result, each node has a higher visit count, improving the precision of its value estimates. 


\subsubsection{MCTS Algorithm}
While the MCTS algorithm is running, it continuously performs the four MCTS steps: selection, expansion, simulation, and backpropagation. The specific implementation details are as follows:
\begin{enumerate}
    \item \textbf{Selection}: The algorithm traverses the game tree, selecting child nodes based on UCB1-tuned, as described by equation~\ref{eq:ucb1}. By adjusting the exploration based on the variance of the outcomes, the agent is able to improve sample efficiency, especially in stochastic games such as backgammon. The best child node is continuously selected until a leaf node is reached. 
    
    \item \textbf{Expansion}: If the child is not terminal, it will have at least one unvisited child node. In this case, a random child node is selected and added to the tree.
    
    \item \textbf{Simulation}: A random playout is then performed until either a predetermined depth limit or a passed state has been reached. If the depth limit is reached first, one of three evaluations are used, based on the chosen evaluation mode:
    \begin{itemize}
        \item \textbf{Default}: The pip count ratio. e.g. if the white player has 10 pips and the black player has 20 pips, the ratio is $\frac{2}{3}$ for white and $\frac{1}{3}$ for black. This is a very fast evaluation, but not very accurate.
        \item \textbf{Heuristic}: The heuristic agent described in section \ref{sec:heuristic} is used to evaluate the board state. This is more accurate than only using the pip counts, but not as accurate as the neural network.
        \item \textbf{Neural Network}: The neural network described in section \ref{sec:neural} is used to evaluate the board state. This is the most accurate method, but requires a trained model.
    \end{itemize}
    The value from the evaluation is then normalised to a value between -1 and 1, where -1 is a loss for the player, 0 is a draw, and 1 is a win. 
    In the case that a passed state is reached, the $D^2/S$ formula is used to predict the winner. There are multiple benefits from this approach:
    \begin{itemize}
    \item By finishing the simulation early, the cost is reduced, allowing more simulations to be performed overall and increasing the accuracy of the statistics.
    \item Full playouts from passed positions involve random dice rolls, which may skew the results. By predicting the probability rather than relying on the law of large numbers, the value of the node has much lower variance.
    \item In a passed state, there are no more interactions between checkers. Random playouts through these positions result in very little strategic information, as the winner from this point is mostly determined by the pip count and chance. By using equation~\ref{eq:d2s}, the MCTS agent does not have to "re-learn" this relationship using repeated rollouts, and can instead directly use the precomputed statistic which it would eventually converge to.
    \end{itemize}

    \item \textbf{Backpropagation}: The result of the simulation is then backpropagated through the tree, updating the statistics of each node in the path. This includes the following values:
    \begin{itemize}
        \item $N$: The number of times the node has been visited.
        \item $Q$: The total reward the node has received.
        \item $Q^2$: The sum of the squares of the rewards received (used in the variance calculation).
    \end{itemize}
\end{enumerate}


\subsection{Neural Network Agent}
\label{sec:neural}
The Neural Network agent is a value function approximator, inspired by TD-Gammon \cite{Tesauro1995}, where the network predicts the probability that white wins a given board state. 
This was chosen as the method, as opposed to the network outputting a move, so the agent would not have to learn the rules of the game, and could instead focus on learning the strategy. The neural network was trained using TD-$\lambda$ learning in pytorch, which allows for strong GPU support and a familiar pythonic syntax.


\paragraph{Feature Representation}
Deep learning models are able to learn features from the raw input data, but in order to guide training, extra features were provided in the input. The following inputs were given to the network:
\begin{itemize}
    \item The number of checkers at each board position (positive for white, negative for black);
    \item The number of checkers borne off for each player;
    \item The number of blots for each player;
    \item The number of anchor points for each player;
    \item The length of the longest prime.
\end{itemize}

\begin{definition}[Vanishing Gradient Problem]
During backpropagation, the gradient of loss functions with respect to each weight is calculated. The gradients of some activation functions are always less than 1 (in the range of (0,1] for tanh and a maximum of 0.25 for sigmoid). The product of these values shrinks exponentially, resulting in earlier layers learning extremely slowly.   
\end{definition}

A multilayer perceptron was chosen where each layer is fully connected with ReLU activation functions. The output layer is a single neuron with a sigmoid activation function, which outputs a value between 0 and 1, corresponding to the probability that white wins. ReLU was chosen as it is extremely computationally cheap to compute and mitigates the vanishing gradient problem, since its derivative is always 1, ensuring that gradients being backpropagated do not shrink \cite{reluvanish}.

A sigmoid function was used in the output layer as its range is between 0 and 1, and is generally preferred for probability outputs \cite{LeCun2012}. 

\paragraph{Temporal Difference Learning}
Training was split into epochs of 500 games which were played concurrently. Despite dice rolls providing natural exploration, an additional exploration rate of 10\% was used, where the agent would pick a random move. 
After the games had been completed, TD-$\lambda$ learning was used to update the weights of the neural network, using a learning rate of $\eta$, and a $\lambda$ value which was provided to the agent at initialisation. A discount factor of $\gamma = 0.99$ was chosen as high values are preferred when applying TD=$\lambda$ learning to games. Using a high $\gamma$ value decreases the bias in the updates, but increases the variance, as the agent has to approximate future rewards at longer distances (i.e. the model is more complex) \cite{BRISCOE20112}. However, the high variance can be mitigated by increasing the number of games in the training data, whereas bias cannot be addressed in the same way, so choosing a model with low bias is less consequential.


An implementation which adjusted learning for momentum was used, however during testing, this was found to have a negative impact in the performance of the agent, as shown in figure TODO.
After each epoch, the neural network was evaluated by playing 500 games against a random agent, split evenly between playing as white and black. 
Once the learning process had been proved to work by being able to beat the random agent almost 100\% of the time, the heursitic agent was used instead of the random agent. Winrates from these evaluations were plotted to see how the agent was improving over time, with the weights being saved every 5 epochs. During training, the neural network was able to play and learn from approximately 4 games per second.

\subsection{Model Architecture Selection}

In order to determine what model architecture the final neural network